<HEAD>
<TITLE></TITLE>
</HEAD>
<BODY>
<H1>GRIDDER CODE FOR MAKING DEMS FROM POINT CLOUDS</H1>
<HR>

<H2>INDEX</H2>
<UL>
<LI><A HREF="#grid_intro">Introduction</A>
<P>
<LI><A HREF="#grid_test">Filetypes Tested Against</A>
<P>
<LI><A HREF="#grid_grid">Implementation</A>
<P>
</UL>

<H2><A NAME="grid_intro">INTRODUCTION</A></H2>

The code was originally written to create DEMS from very large collections of point-cloud files.
It was used to make maps for Woson North Korea and for Montreal.
It has the capacity to process hundreds of point-cloud files at a time and create very large maps made up of multiple DEM tiles.
<P>
The gridding technique used by BuckEye and others was very poor for our purposes and the approach used here is much better
(see various Dammann papers and symposium presentations).
<P>
The code has been tested on relatively clean .las point-cloud files (as would be generated by a good linear-mode lidar).
Only points that are very far from the average surface and that are certainly noise are filtered out.
It has also been tested on a few very particular .bpf point-cloud files that have been processed by the FINE algorithm (see below).
It the input filetype is .bpf, it is assumed that it is this subtype and processed accordingly.
<P>



<H2><A NAME="grid_test">FILE TYPES TESTED</A></H2>

<H3><A NAME="las_test">LAS Files </A></H3>

I have tested against .las files generated by the BuckEye program, and a variety of other datasets generated by different lidars.
I have also used files generated by Damon Conover of this branch using photogrammetry.
<P>

<H3><A NAME="las_test">BPF Files </A></H3>

I received a number of .bpf point-cloud files from a government source that was testing an algorithm called FINE.
This algorithm attaches a quality metric to each return in the point cloud.
This is a non-trivial algorithm that attempts to associate points with larger features so generates a better quality metric than
simply counting returns from a spot.
All the points are kept so that a threshold can be adjusted up or down depending on the application.
Adjusting down eliminates noise but may result in losing some faint features.
When gridding it is necessary to turn this threshold pretty far down to eliminate virtually all the noise.
Adjusting up makes more features visible but introduces a lot of noise.
It is most useful for visual display for humans, since we are so good at seeing subtle patterns.
<P>
I have only implemented gridder for .bpf files using FINE since those are the only files I have to test against.
If other kinds of .bpf files need to be gridded, then the code must be modified.
<P>

<PRE>


	  thresh_avg = 2.f;			// Camp Roberts, NTS			probably doesnt matter
	  thresh_avg = 3.f;			// Haiti, elYunque, pender

	  thresh_tau = 70;			// Kabul
	  thresh_tau = 70;			// Village
	  thresh_tau = 63;			// Powerlines
	  thresh_tau = 85;			// Spiderholes
	  thresh_tau = 80;			// Snowfield


</PRE>

<H2><A NAME="grid_grid">GRIDDING IMPLEMENTATION</A></H2>

<H3><A NAME="las_grid">Crashes in Debug Mode</A></H3>

The code sometimes crashed when run in Debug mode.
The issue appeared to be with GDAL and have to do with the GDAL class OGRSpatialReference.
There were no issues in Release mode.
GDAL provides executables for Windows only for Release mode, so this may be what causes the problem.
<P>

<H3><A NAME="las_grid">Call Format </A></H3>

A virtually unlimited number of point-cloud files can be input.
<P>
The output must have a coordinate system defined by an EPSG number.
Most input point clouds have metadata that includes an EPSG number.
In this case, the number will just be transferred to the output file.
In the event that the ESPG number is not in the input files, it must be entered from the menu.
<P>
